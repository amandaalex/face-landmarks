{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Landmarks Detection\n",
    "\n",
    "Detect face landmarks and facial expressions in images and videos.\n",
    "\n",
    "The task outputs 3-dimensional face landmarks, blendshape scores (coefficients representing facial expression) to infer detailed facial surfaces in real-time, and transformation matrices to perform the transformations required for effects rendering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imutils\n",
    "import matplotlib.image as mpimg\n",
    "from collections import OrderedDict\n",
    "from skimage import io, transform\n",
    "from math import *\n",
    "import xml.etree.ElementTree as ET \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the DLIB Dataset\n",
    "\n",
    "An official DLIB dataset which consists of over 6666 images of different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "cell": {
        "!": "OSMagics",
        "HTML": "Other",
        "SVG": "Other",
        "bash": "Other",
        "capture": "ExecutionMagics",
        "code_wrap": "ExecutionMagics",
        "debug": "ExecutionMagics",
        "file": "Other",
        "html": "DisplayMagics",
        "javascript": "DisplayMagics",
        "js": "DisplayMagics",
        "latex": "DisplayMagics",
        "markdown": "DisplayMagics",
        "perl": "Other",
        "prun": "ExecutionMagics",
        "pypy": "Other",
        "python": "Other",
        "python2": "Other",
        "python3": "Other",
        "ruby": "Other",
        "script": "ScriptMagics",
        "sh": "Other",
        "svg": "DisplayMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "writefile": "OSMagics"
       },
       "line": {
        "alias": "OSMagics",
        "alias_magic": "BasicMagics",
        "autoawait": "AsyncMagics",
        "autocall": "AutoMagics",
        "automagic": "AutoMagics",
        "autosave": "KernelMagics",
        "bookmark": "OSMagics",
        "cat": "Other",
        "cd": "OSMagics",
        "clear": "KernelMagics",
        "code_wrap": "ExecutionMagics",
        "colors": "BasicMagics",
        "conda": "PackagingMagics",
        "config": "ConfigMagics",
        "connect_info": "KernelMagics",
        "cp": "Other",
        "debug": "ExecutionMagics",
        "dhist": "OSMagics",
        "dirs": "OSMagics",
        "doctest_mode": "BasicMagics",
        "ed": "Other",
        "edit": "KernelMagics",
        "env": "OSMagics",
        "gui": "BasicMagics",
        "hist": "Other",
        "history": "HistoryMagics",
        "killbgscripts": "ScriptMagics",
        "ldir": "Other",
        "less": "KernelMagics",
        "lf": "Other",
        "lk": "Other",
        "ll": "Other",
        "load": "CodeMagics",
        "load_ext": "ExtensionMagics",
        "loadpy": "CodeMagics",
        "logoff": "LoggingMagics",
        "logon": "LoggingMagics",
        "logstart": "LoggingMagics",
        "logstate": "LoggingMagics",
        "logstop": "LoggingMagics",
        "ls": "Other",
        "lsmagic": "BasicMagics",
        "lx": "Other",
        "macro": "ExecutionMagics",
        "magic": "BasicMagics",
        "mamba": "PackagingMagics",
        "man": "KernelMagics",
        "matplotlib": "PylabMagics",
        "micromamba": "PackagingMagics",
        "mkdir": "Other",
        "more": "KernelMagics",
        "mv": "Other",
        "notebook": "BasicMagics",
        "page": "BasicMagics",
        "pastebin": "CodeMagics",
        "pdb": "ExecutionMagics",
        "pdef": "NamespaceMagics",
        "pdoc": "NamespaceMagics",
        "pfile": "NamespaceMagics",
        "pinfo": "NamespaceMagics",
        "pinfo2": "NamespaceMagics",
        "pip": "PackagingMagics",
        "popd": "OSMagics",
        "pprint": "BasicMagics",
        "precision": "BasicMagics",
        "prun": "ExecutionMagics",
        "psearch": "NamespaceMagics",
        "psource": "NamespaceMagics",
        "pushd": "OSMagics",
        "pwd": "OSMagics",
        "pycat": "OSMagics",
        "pylab": "PylabMagics",
        "qtconsole": "KernelMagics",
        "quickref": "BasicMagics",
        "recall": "HistoryMagics",
        "rehashx": "OSMagics",
        "reload_ext": "ExtensionMagics",
        "rep": "Other",
        "rerun": "HistoryMagics",
        "reset": "NamespaceMagics",
        "reset_selective": "NamespaceMagics",
        "rm": "Other",
        "rmdir": "Other",
        "run": "ExecutionMagics",
        "save": "CodeMagics",
        "sc": "OSMagics",
        "set_env": "OSMagics",
        "store": "StoreMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "tb": "ExecutionMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "unalias": "OSMagics",
        "unload_ext": "ExtensionMagics",
        "who": "NamespaceMagics",
        "who_ls": "NamespaceMagics",
        "whos": "NamespaceMagics",
        "xdel": "NamespaceMagics",
        "xmode": "BasicMagics"
       }
      },
      "text/plain": [
       "Available line magics:\n",
       "%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %code_wrap  %colors  %conda  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %mamba  %man  %matplotlib  %micromamba  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n",
       "\n",
       "Available cell magics:\n",
       "%%!  %%HTML  %%SVG  %%bash  %%capture  %%code_wrap  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n",
       "\n",
       "Automagic is ON, % prefix IS NOT needed for line magics."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%%capture\n",
    "if not os.path.exists('/content/ibug_300W_large_face_landmark_dataset'):\n",
    "    !wget http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz\n",
    "    !tar -xvzf 'ibug_300W_large_face_landmark_dataset.tar.gz'    \n",
    "    !rm -r 'ibug_300W_large_face_landmark_dataset.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "# Create a StringIO object to capture the output\n",
    "capture_output = StringIO()\n",
    "\n",
    "# Redirect the standard output to the StringIO object\n",
    "sys.stdout = capture_output\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists('/content/ibug_300W_large_face_landmark_dataset'):\n",
    "    # Download the dataset\n",
    "    !wget http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz\n",
    "    \n",
    "    # Extract the dataset\n",
    "    !tar -xvzf 'ibug_300W_large_face_landmark_dataset.tar.gz'\n",
    "    \n",
    "    # Remove the downloaded archive\n",
    "    !rm -r 'ibug_300W_large_face_landmark_dataset.tar.gz'\n",
    "\n",
    "# Get the captured output as a string\n",
    "captured_text = capture_output.getvalue()\n",
    "\n",
    "# Reset the standard output to its original value\n",
    "sys.stdout = sys.__stdout__\n",
    "\n",
    "# Print the captured output\n",
    "print(\"Captured output:\")\n",
    "print(captured_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured output:\n",
      "/bin/bash: wget: command not found\n",
      "tar: Error opening archive: Failed to open 'ibug_300W_large_face_landmark_dataset.tar.gz'\n",
      "rm: ibug_300W_large_face_landmark_dataset.tar.gz: No such file or directory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "# Create a StringIO object to capture the output\n",
    "capture_output = StringIO()\n",
    "\n",
    "# Redirect the standard output to the StringIO object\n",
    "sys.stdout = capture_output\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists('/content/ibug_300W_large_face_landmark_dataset'):\n",
    "    # Download the dataset\n",
    "    !wget http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz\n",
    "    \n",
    "    # Extract the dataset\n",
    "    !tar -xvzf 'ibug_300W_large_face_landmark_dataset.tar.gz'\n",
    "    \n",
    "    # Remove the downloaded archive\n",
    "    !rm -r 'ibug_300W_large_face_landmark_dataset.tar.gz'\n",
    "\n",
    "# Get the captured output as a string\n",
    "captured_text = capture_output.getvalue()\n",
    "\n",
    "# Reset the standard output to its original value\n",
    "sys.stdout = sys.__stdout__\n",
    "\n",
    "# Print the captured output\n",
    "print(\"Captured output:\")\n",
    "print(captured_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Dataset\n",
    "View the dataset to identify all the data cleaning and preprocessing opportunities that we need to go through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('ibug_300W_large_face_landmark_dataset/helen/trainset/100032540_1.pts')\n",
    "points = file.readlines()[3:-1]\n",
    "\n",
    "landmarks = []\n",
    "\n",
    "for point in points:\n",
    "    x,y = point.split(' ')\n",
    "    landmarks.append([floor(float(x)), floor(float(y[:-1]))])\n",
    "\n",
    "landmarks = np.array(landmarks)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(mpimg.imread('ibug_300W_large_face_landmark_dataset/helen/trainset/100032540_1.jpg'))\n",
    "plt.scatter(landmarks[:,0], landmarks[:,1], s = 5, c = 'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset Classes\n",
    "\n",
    "Review the classes and labels in the dataset. The labels_ibug_300W_train.xml consists of the input images and landmarks and bounding box to crop the face. We can store all these values in the list so that we could easily access them during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transforms():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def rotate(self, image, landmarks, angle):\n",
    "        angle = random.uniform(-angle, +angle)\n",
    "\n",
    "        transformation_matrix = torch.tensor([\n",
    "            [+cos(radians(angle)), -sin(radians(angle))], \n",
    "            [+sin(radians(angle)), +cos(radians(angle))]\n",
    "        ])\n",
    "\n",
    "        image = imutils.rotate(np.array(image), angle)\n",
    "\n",
    "        landmarks = landmarks - 0.5\n",
    "        new_landmarks = np.matmul(landmarks, transformation_matrix)\n",
    "        new_landmarks = new_landmarks + 0.5\n",
    "        return Image.fromarray(image), new_landmarks\n",
    "\n",
    "    def resize(self, image, landmarks, img_size):\n",
    "        image = TF.resize(image, img_size)\n",
    "        return image, landmarks\n",
    "\n",
    "    def color_jitter(self, image, landmarks):\n",
    "        color_jitter = transforms.ColorJitter(brightness=0.3, \n",
    "                                              contrast=0.3,\n",
    "                                              saturation=0.3, \n",
    "                                              hue=0.1)\n",
    "        image = color_jitter(image)\n",
    "        return image, landmarks\n",
    "\n",
    "    def crop_face(self, image, landmarks, crops):\n",
    "        left = int(crops['left'])\n",
    "        top = int(crops['top'])\n",
    "        width = int(crops['width'])\n",
    "        height = int(crops['height'])\n",
    "\n",
    "        image = TF.crop(image, top, left, height, width)\n",
    "\n",
    "        img_shape = np.array(image).shape\n",
    "        landmarks = torch.tensor(landmarks) - torch.tensor([[left, top]])\n",
    "        landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]])\n",
    "        return image, landmarks\n",
    "\n",
    "    def __call__(self, image, landmarks, crops):\n",
    "        image = Image.fromarray(image)\n",
    "        image, landmarks = self.crop_face(image, landmarks, crops)\n",
    "        image, landmarks = self.resize(image, landmarks, (224, 224))\n",
    "        image, landmarks = self.color_jitter(image, landmarks)\n",
    "        image, landmarks = self.rotate(image, landmarks, angle=10)\n",
    "        \n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, [0.5], [0.5])\n",
    "        return image, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceLandmarksDataset(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "\n",
    "        tree = ET.parse('ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml')\n",
    "        root = tree.getroot()\n",
    "\n",
    "        self.image_filenames = []\n",
    "        self.landmarks = []\n",
    "        self.crops = []\n",
    "        self.transform = transform\n",
    "        self.root_dir = 'ibug_300W_large_face_landmark_dataset'\n",
    "        \n",
    "        for filename in root[2]:\n",
    "            self.image_filenames.append(os.path.join(self.root_dir, filename.attrib['file']))\n",
    "\n",
    "            self.crops.append(filename[0].attrib)\n",
    "\n",
    "            landmark = []\n",
    "            for num in range(68):\n",
    "                x_coordinate = int(filename[0][num].attrib['x'])\n",
    "                y_coordinate = int(filename[0][num].attrib['y'])\n",
    "                landmark.append([x_coordinate, y_coordinate])\n",
    "            self.landmarks.append(landmark)\n",
    "\n",
    "        self.landmarks = np.array(self.landmarks).astype('float32')     \n",
    "\n",
    "        assert len(self.image_filenames) == len(self.landmarks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.image_filenames[index], 0)\n",
    "        landmarks = self.landmarks[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            image, landmarks = self.transform(image, landmarks, self.crops[index])\n",
    "\n",
    "        landmarks = landmarks - 0.5\n",
    "\n",
    "        return image, landmarks\n",
    "\n",
    "dataset = FaceLandmarksDataset(Transforms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Train Transforms\n",
    "\n",
    "Visualize the dataset by performing the transformation using the above classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, landmarks = dataset[0]\n",
    "landmarks = (landmarks + 0.5) * 224\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image.numpy().squeeze(), cmap='gray');\n",
    "plt.scatter(landmarks[:,0], landmarks[:,1], s=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Dataset for Training and Prediction of Face Landmarks\n",
    "\n",
    "Split the dataset into a train and a valid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into validation and test sets\n",
    "len_valid_set = int(0.1*len(dataset))\n",
    "len_train_set = len(dataset) - len_valid_set\n",
    "\n",
    "print(\"The length of Train set is {}\".format(len_train_set))\n",
    "print(\"The length of Valid set is {}\".format(len_valid_set))\n",
    "\n",
    "train_dataset , valid_dataset,  = torch.utils.data.random_split(dataset , [len_train_set, len_valid_set])\n",
    "\n",
    "# shuffle and batch the datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=8, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the shape of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, landmarks = next(iter(train_loader))\n",
    "\n",
    "print(images.shape)\n",
    "print(landmarks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Face Landmarks Detection Model\n",
    "Use the ResNet18 as our fundamental framework. \n",
    "Modify the first and last layers so that the layers will fit easily for the intended purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,num_classes=136):\n",
    "        super().__init__()\n",
    "        self.model_name='resnet18'\n",
    "        self.model=models.resnet18()\n",
    "        self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.fc=nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def print_overwrite(step, total_step, loss, operation):\n",
    "    sys.stdout.write('\\r')\n",
    "    if operation == 'train':\n",
    "        sys.stdout.write(\"Train Steps: %d/%d  Loss: %.4f \" % (step, total_step, loss))   \n",
    "    else:\n",
    "        sys.stdout.write(\"Valid Steps: %d/%d  Loss: %.4f \" % (step, total_step, loss))\n",
    "        \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network for Face Landmarks Detection\n",
    "\n",
    "Use the Mean Squared Error between the true and predicted face Landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "network = Network()\n",
    "network.cuda()    \n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.0001)\n",
    "\n",
    "loss_min = np.inf\n",
    "num_epochs = 10\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    \n",
    "    loss_train = 0\n",
    "    loss_valid = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    network.train()\n",
    "    for step in range(1,len(train_loader)+1):\n",
    "    \n",
    "        images, landmarks = next(iter(train_loader))\n",
    "        \n",
    "        images = images.cuda()\n",
    "        landmarks = landmarks.view(landmarks.size(0),-1).cuda() \n",
    "        \n",
    "        predictions = network(images)\n",
    "        \n",
    "        # clear all the gradients before calculating them\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # find the loss for the current step\n",
    "        loss_train_step = criterion(predictions, landmarks)\n",
    "        \n",
    "        # calculate the gradients\n",
    "        loss_train_step.backward()\n",
    "        \n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_train += loss_train_step.item()\n",
    "        running_loss = loss_train/step\n",
    "        \n",
    "        print_overwrite(step, len(train_loader), running_loss, 'train')\n",
    "        \n",
    "    network.eval() \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for step in range(1,len(valid_loader)+1):\n",
    "            \n",
    "            images, landmarks = next(iter(valid_loader))\n",
    "        \n",
    "            images = images.cuda()\n",
    "            landmarks = landmarks.view(landmarks.size(0),-1).cuda()\n",
    "        \n",
    "            predictions = network(images)\n",
    "\n",
    "            # find the loss for the current step\n",
    "            loss_valid_step = criterion(predictions, landmarks)\n",
    "\n",
    "            loss_valid += loss_valid_step.item()\n",
    "            running_loss = loss_valid/step\n",
    "\n",
    "            print_overwrite(step, len(valid_loader), running_loss, 'valid')\n",
    "    \n",
    "    loss_train /= len(train_loader)\n",
    "    loss_valid /= len(valid_loader)\n",
    "    \n",
    "    print('\\n--------------------------------------------------')\n",
    "    print('Epoch: {}  Train Loss: {:.4f}  Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid))\n",
    "    print('--------------------------------------------------')\n",
    "    \n",
    "    if loss_valid < loss_min:\n",
    "        loss_min = loss_valid\n",
    "        torch.save(network.state_dict(), '/content/face_landmarks.pth') \n",
    "        print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, num_epochs))\n",
    "        print('Model Saved\\n')\n",
    "print('Training Complete')\n",
    "print(\"Total Elapsed Time : {} s\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Landmarks Prediction\n",
    "\n",
    "Use the model that we trained above on the unseen images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    best_network = Network()\n",
    "    best_network.cuda()\n",
    "    best_network.load_state_dict(torch.load('/content/face_landmarks.pth')) \n",
    "    best_network.eval()\n",
    "    \n",
    "    images, landmarks = next(iter(valid_loader))\n",
    "    \n",
    "    images = images.cuda()\n",
    "    landmarks = (landmarks + 0.5) * 224\n",
    "\n",
    "    predictions = (best_network(images).cpu() + 0.5) * 224\n",
    "    predictions = predictions.view(-1,68,2)\n",
    "    \n",
    "    plt.figure(figsize=(10,40))\n",
    "    \n",
    "    for img_num in range(8):\n",
    "        plt.subplot(8,1,img_num+1)\n",
    "        plt.imshow(images[img_num].cpu().numpy().transpose(1,2,0).squeeze(), cmap='gray')\n",
    "        plt.scatter(predictions[img_num,:,0], predictions[img_num,:,1], c = 'r', s = 5)\n",
    "        plt.scatter(landmarks[img_num,:,0], landmarks[img_num,:,1], c = 'g', s = 5)\n",
    "\n",
    "print('Total number of test images: {}'.format(len(valid_dataset)))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Elapsed Time : {}\".format(end_time - start_time)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
